{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on Fri Mar 15 19:43:56 2019\n",
    "\n",
    "@author: rajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############### Load Libraries\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "# Init\n",
    "data_dir = \"../UCF-101_video_classification-master/data\"\n",
    "features = 2048\n",
    "batch_size = 32\n",
    "seq_len = 40\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0005\n",
    "weight_decay  = 0.0005\n",
    "# Detect if we have a GPU available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "#Train module\n",
    "def train_model(model, dataloaders_dict,criterion, optimizer, scheduler, \n",
    "                num_epochs = 25,\n",
    "                steps_per_epoch = 100,\n",
    "                val_steps = 100,\n",
    "                stateful = False):\n",
    "\n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            itercnt = 0\n",
    "\n",
    "            for inputs, labels in dataloaders_dict[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if stateful:\n",
    "                    model.hidden = model.init_hidden()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()                    \n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                itercnt+=1\n",
    "                if(phase == 'train'):                    \n",
    "                    if(itercnt > steps_per_epoch):                                            \n",
    "                        break\n",
    "                else:\n",
    "                    if(itercnt > val_steps):                                            \n",
    "                        break                    \n",
    "\n",
    "    #            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "    #            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            if(phase == 'train'):  \n",
    "                epoch_loss = running_loss / (itercnt * batch_size)\n",
    "                epoch_acc = running_corrects.double() / (itercnt * batch_size)\n",
    "            else:\n",
    "                epoch_loss = running_loss / (itercnt * batch_size)\n",
    "                epoch_acc = running_corrects.double() / (itercnt * batch_size)\n",
    "\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "import glob\n",
    "train_seq = sorted(glob.glob( data_dir + '/train-full/[A-B]**/*.avi', recursive = True))\n",
    "val_seq   = sorted(glob.glob( data_dir + '/val-full/[A-B]**/*.avi', recursive = True))\n",
    "features_seq  = sorted(glob.glob(data_dir + '/sequences/*.npy'))\n",
    "\n",
    "print( len(train_seq))\n",
    "print( len(val_seq))\n",
    "print( len(features_seq))\n",
    "\n",
    "import csv\n",
    "with open( data_dir + '/data_file.csv', 'r') as fin:\n",
    "    reader = csv.reader(fin)\n",
    "    data = list(reader)\n",
    "\n",
    "classes = []\n",
    "for item in data:\n",
    "    if item[1] not in classes:\n",
    "        classes.append(item[1])\n",
    "classes = sorted(classes)\n",
    "num_classes = len(classes)\n",
    "\n",
    "data_clean = []\n",
    "max_frames = 300\n",
    "for item in data:\n",
    "    if int(item[3]) >= seq_len and int(item[3]) <= max_frames\\\n",
    "            and item[1] in classes:\n",
    "        data_clean.append(item)\n",
    "print(len(data))\n",
    "data = data_clean\n",
    "print(len(data))\n",
    "\n",
    "train = []\n",
    "val = []\n",
    "for item in data:\n",
    "    if item[0] == 'train':\n",
    "        train.append(item)\n",
    "    else:\n",
    "        val.append(item)\n",
    "\n",
    "import random\n",
    "def frame_generator(batch_Size, train_test):\n",
    "    data = train if train_test == 'train' else val\n",
    "\n",
    "    while 1:\n",
    "        X, ye = [], []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            sequence = None        \n",
    "            row = random.choice(data)\n",
    "\n",
    "            path = '../UCF-101_video_classification-master/data/sequences/'+\\\n",
    "                    row[2] + '-' + str(seq_len) + '-' + 'features.npy'        \n",
    "\n",
    "            if os.path.isfile(path):\n",
    "                sequence = np.load(path)            \n",
    "            else:\n",
    "                sequence = None\n",
    "\n",
    "            if sequence is None:\n",
    "                print(row)\n",
    "                print(path)\n",
    "                print(\"Can't find sequence. Did you generate them?\")\n",
    "                return [],[]\n",
    "\n",
    "\n",
    "            X.append(sequence)\n",
    "            label_encoded = classes.index(row[1])\n",
    "#            label_hot = np.eye(len(classes), dtype='float32')[label_encoded]              \n",
    "#            y.append(label_hot)\n",
    "            ye.append(label_encoded)\n",
    "\n",
    "        ye = np.array(ye)\n",
    "\n",
    "        X = np.array(X)\n",
    "        X = X.squeeze(2)\n",
    "        X = X.transpose(1,0,2)\n",
    "\n",
    "        X = torch.tensor(X)\n",
    "        ye = torch.tensor(ye)\n",
    "\n",
    "        yield X,ye\n",
    "\n",
    "#Xtrain, ytrain = get_all_sequences_in_memory(batch_size, 'train')\n",
    "#Xval, yval = get_all_sequences_in_memory(batch_size, 'val')\n",
    "\n",
    "\n",
    "dataloaders_dict = {x: frame_generator(batch_size, x) \n",
    "                    for x in ['train', 'val']\n",
    "                    }\n",
    "\n",
    "########## MODEL\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, features, batch_size, classes, NumFutureSteps):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_dim = features\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = 1\n",
    "        self.output_dim = classes\n",
    "        self.hidden_dim = 4096\n",
    "        self.hidden_dec_dim = 4096\n",
    "        self.input_dec_dim = 512\n",
    "\n",
    "        self.NumFutureSteps = NumFutureSteps\n",
    "\n",
    "        self.r_0 = torch.zeros(self.batch_size, self.input_dec_dim)\n",
    "        self.fc1 = nn.Linear(self.hidden_dim, self.hidden_dec_dim)\n",
    "\n",
    "        self.lstmcell_decoder = nn.LSTMCell(self.input_dec_dim, self.hidden_dim)\n",
    "#        self.lstm2 = nn.LSTM(512, classes, 1)\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "#        self.lstm3 = nn.LSTM(features, classes, 2)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        outputs = []        \n",
    "        for i, input_t in enumerate(x.chunk(x.size(2)*x.size(1), dim = 0)):           \n",
    "            input_t = input_t.squeeze()            \n",
    "            self.hidden = self.lstm1(input_t, self.hidden)            \n",
    "            output = self.linear(self.hidden[0])            \n",
    "            outputs += [output]           \n",
    "        outputs = torch.stack(outputs, 0)\n",
    "\n",
    "#        for i in range(future):# if we should predict the future\n",
    "#            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "#            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "#            output = self.linear(h_t2)\n",
    "#            outputs += [output]\n",
    "#        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return output\n",
    "\n",
    "#        o1, self.hidden = self.lstm1(x) \n",
    "##        o2, (h2,c2) = self.lstm2(o1) \n",
    "##        o2, (h1,c1) = self.lstm3(x) \n",
    "#        outputs = self.linear(o1[-1])\n",
    "#        return outputs\n",
    "\n",
    "    def init_hidden(self):\n",
    "        self.r_0 = torch.zeros(self.batch_size, self.input_dec_dim)\n",
    "        return (torch.zeros(self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.batch_size, self.hidden_dim))\n",
    "\n",
    "model = Model(features, batch_size, num_classes, seq_len)\n",
    "print(model)\n",
    "\n",
    "## Training and Validation\n",
    "params_to_update = model.parameters()\n",
    "\n",
    "count = 0\n",
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad == True:        \n",
    "        print(\"\\t\",name)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('No of trainable parameters', params)\n",
    "\n",
    "optimizer = optim.Adam(params_to_update,\n",
    "                 lr = learning_rate,\n",
    "                 betas = (0.9, 0.999), \n",
    "                 eps = 1e-08, \n",
    "                 weight_decay = weight_decay, \n",
    "                 amsgrad=False)\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "\n",
    "#model_best, hist = train_model(model, dataloaders_dict,\n",
    "#                             criterion, optimizer, scheduler, \n",
    "#                             num_epochs = num_epochs,\n",
    "#                             steps_per_epoch = int(len(train)/batch_size),\n",
    "#                             val_steps = int(len(val)/batch_size),\n",
    "#                             stateful = False\n",
    "#                             )\n",
    "\n",
    "since = time.time()\n",
    "val_acc_history = []\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        itercnt = 0\n",
    "\n",
    "        for inputs, labels in dataloaders_dict[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if stateful:\n",
    "                model.hidden = model.init_hidden()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()                    \n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            itercnt+=1\n",
    "            if(phase == 'train'):                    \n",
    "                if(itercnt > steps_per_epoch):                                            \n",
    "                    break\n",
    "            else:\n",
    "                if(itercnt > val_steps):                                            \n",
    "                    break                    \n",
    "\n",
    "#            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "        if(phase == 'train'):  \n",
    "            epoch_loss = running_loss / (itercnt * batch_size)\n",
    "            epoch_acc = running_corrects.double() / (itercnt * batch_size)\n",
    "        else:\n",
    "            epoch_loss = running_loss / (itercnt * batch_size)\n",
    "            epoch_acc = running_corrects.double() / (itercnt * batch_size)\n",
    "\n",
    "\n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "        # deep copy the model\n",
    "        if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if phase == 'val':\n",
    "            val_acc_history.append(epoch_acc)\n",
    "\n",
    "    print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
